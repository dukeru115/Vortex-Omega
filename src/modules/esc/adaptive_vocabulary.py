"""
Adaptive Vocabulary System for ESC Module 2.1

Advanced vocabulary adaptation with constitutional constraints:
- Dynamic vocabulary expansion with safety validation
- Constitutional token evaluation and scoring
- Frequency-based learning with risk assessment
- Emergency vocabulary lockdown and safe-word enforcement
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Set
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict, Counter
import logging
import re
import hashlib
import time

logger = logging.getLogger(__name__)


class VocabularyMode(Enum):
    """Vocabulary adaptation modes."""
    STATIC = "static"          # No vocabulary expansion
    CONSERVATIVE = "conservative"  # Very careful expansion
    BALANCED = "balanced"      # Standard expansion
    EXPLORATORY = "exploratory"  # Aggressive expansion
    EMERGENCY = "emergency"    # Emergency lockdown


class TokenStatus(Enum):
    """Token status in adaptive vocabulary."""
    APPROVED = "approved"
    PENDING = "pending"
    REJECTED = "rejected"
    QUARANTINE = "quarantine"
    CONSTITUTIONAL = "constitutional"


@dataclass
class VocabularyConfig:
    """Configuration for adaptive vocabulary system."""
    mode: VocabularyMode = VocabularyMode.BALANCED
    base_vocabulary_size: int = 50000
    max_expansion_size: int = 10000
    min_frequency_threshold: int = 5
    constitutional_score_threshold: float = 0.6
    risk_score_threshold: float = 0.7
    adaptation_rate: float = 0.01
    quarantine_period: int = 100  # Number of encounters before re-evaluation


@dataclass
class TokenEntry:
    """Entry for a token in adaptive vocabulary."""
    token: str
    token_id: int
    frequency: int = 0
    constitutional_score: float = 0.0
    risk_score: float = 0.0
    status: TokenStatus = TokenStatus.PENDING
    first_seen: float = 0.0
    last_seen: float = 0.0
    contexts: List[str] = field(default_factory=list)
    quarantine_count: int = 0


class AdaptiveVocabulary:
    """
    Adaptive Vocabulary System for Constitutional ESC.
    
    Manages dynamic vocabulary expansion with constitutional safety constraints,
    learning from usage patterns while maintaining safety and compliance.
    """
    
    def __init__(self, config: VocabularyConfig):
        """
        Initialize adaptive vocabulary system.
        
        Args:
            config: Vocabulary configuration
        """
        self.config = config
        self.current_mode = config.mode
        
        # Core vocabulary storage
        self.base_vocabulary: Dict[str, TokenEntry] = {}
        self.expanded_vocabulary: Dict[str, TokenEntry] = {}
        self.rejected_tokens: Dict[str, TokenEntry] = {}
        self.quarantine_tokens: Dict[str, TokenEntry] = {}
        
        # Token tracking
        self.token_id_counter = config.base_vocabulary_size
        self.token_usage_history = defaultdict(list)
        self.context_associations = defaultdict(set)
        
        # Constitutional monitoring
        self.constitutional_violations = []
        self.safety_interventions = []
        self.adaptation_history = []
        
        # Statistics
        self.stats = {
            'tokens_evaluated': 0,
            'tokens_approved': 0,
            'tokens_rejected': 0,
            'constitutional_blocks': 0,
            'emergency_lockdowns': 0,\n            'vocabulary_resets': 0\n        }\n        \n        # Initialize constitutional patterns\n        self._initialize_constitutional_patterns()\n        self._initialize_safety_filters()\n        \n        logger.info(f\"Adaptive vocabulary system initialized\")\n        logger.info(f\"Mode: {config.mode.value}, Base size: {config.base_vocabulary_size}\")\n    \n    def _initialize_constitutional_patterns(self):\n        \"\"\"Initialize constitutional evaluation patterns.\"\"\"\n        # Constitutional positive patterns\n        self.constitutional_positive = {\n            'safety': [r'\\b(safe|secure|protect|privacy|ethical)\\w*'],\n            'education': [r'\\b(learn|teach|educate|inform|knowledge)\\w*'],\n            'help': [r'\\b(help|assist|support|guide|beneficial)\\w*'],\n            'truth': [r'\\b(true|honest|accurate|factual|reliable)\\w*'],\n            'respect': [r'\\b(respect|dignity|fair|equal|inclusive)\\w*']\n        }\n        \n        # Constitutional negative patterns\n        self.constitutional_negative = {\n            'violence': [r'\\b(violence|harm|attack|destroy|kill)\\w*'],\n            'hate': [r'\\b(hate|racist|discriminat|bigot|supremac)\\w*'],\n            'deception': [r'\\b(lie|deceive|fraud|fake|manipulat)\\w*'],\n            'illegal': [r'\\b(illegal|criminal|unlawful|forbidden)\\w*'],\n            'harmful': [r'\\b(toxic|poison|dangerous|lethal|deadly)\\w*']\n        }\n        \n        # Safe core vocabulary (always allowed)\n        self.safe_core_words = {\n            'basic': ['the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'was', 'were'],\n            'pronouns': ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her'],\n            'numbers': ['one', 'two', 'three', 'four', 'five', 'zero', 'first', 'second'],\n            'actions': ['go', 'come', 'see', 'look', 'think', 'know', 'say', 'tell'],\n            'positive': ['good', 'nice', 'kind', 'helpful', 'safe', 'happy', 'thank']\n        }\n    \n    def _initialize_safety_filters(self):\n        \"\"\"Initialize safety filtering systems.\"\"\"\n        # Character-based safety checks\n        self.unsafe_char_patterns = [\n            r'[\\x00-\\x1f]',  # Control characters\n            r'[^\\w\\s\\-\\'\".,!?;:()\\[\\]{}]',  # Unusual characters\n            r'(.)\\1{5,}',  # Excessive repetition\n        ]\n        \n        # Length and format constraints\n        self.format_constraints = {\n            'min_length': 1,\n            'max_length': 50,\n            'max_digits': 0.5,  # Max 50% digits\n            'max_uppercase': 0.8,  # Max 80% uppercase\n            'no_excessive_punctuation': True\n        }\n        \n        # Contextual risk factors\n        self.risk_factors = {\n            'excessive_length': 0.3,\n            'unusual_characters': 0.5,\n            'excessive_repetition': 0.4,\n            'all_caps': 0.2,\n            'mixed_scripts': 0.3,\n            'potential_encoding': 0.6\n        }\n    \n    def evaluate_token(self, \n                      token: str, \n                      context: Optional[str] = None,\n                      force_evaluation: bool = False) -> Tuple[TokenEntry, Dict[str, Any]]:\n        \"\"\"Evaluate a token for inclusion in adaptive vocabulary.\n        \n        Args:\n            token: Token to evaluate\n            context: Optional context where token appeared\n            force_evaluation: Force re-evaluation even if already processed\n            \n        Returns:\n            Tuple of (token_entry, evaluation_info)\n        \"\"\"\n        self.stats['tokens_evaluated'] += 1\n        current_time = time.time()\n        \n        # Check if token already exists\n        existing_entry = self._find_existing_token(token)\n        if existing_entry and not force_evaluation:\n            # Update existing entry\n            existing_entry.frequency += 1\n            existing_entry.last_seen = current_time\n            if context:\n                existing_entry.contexts.append(context[:100])  # Limit context length\n                if len(existing_entry.contexts) > 20:\n                    existing_entry.contexts = existing_entry.contexts[-20:]  # Keep recent contexts\n            \n            evaluation_info = {\n                'is_new_token': False,\n                'existing_status': existing_entry.status.value,\n                'frequency_updated': existing_entry.frequency\n            }\n            return existing_entry, evaluation_info\n        \n        # Create new token entry\n        token_entry = TokenEntry(\n            token=token,\n            token_id=self._generate_token_id(token),\n            frequency=1,\n            first_seen=current_time,\n            last_seen=current_time,\n            contexts=[context[:100]] if context else []\n        )\n        \n        # Evaluate constitutional compliance\n        constitutional_analysis = self._evaluate_constitutional_compliance(token, context)\n        token_entry.constitutional_score = constitutional_analysis['score']\n        \n        # Evaluate safety risks\n        safety_analysis = self._evaluate_safety_risks(token, context)\n        token_entry.risk_score = safety_analysis['risk_score']\n        \n        # Determine token status based on current mode\n        status_decision = self._determine_token_status(\n            token_entry, constitutional_analysis, safety_analysis\n        )\n        token_entry.status = status_decision['status']\n        \n        # Store token in appropriate collection\n        self._store_token(token_entry)\n        \n        # Update statistics\n        if token_entry.status == TokenStatus.APPROVED:\n            self.stats['tokens_approved'] += 1\n        elif token_entry.status == TokenStatus.REJECTED:\n            self.stats['tokens_rejected'] += 1\n        elif token_entry.status == TokenStatus.QUARANTINE:\n            self.stats['constitutional_blocks'] += 1\n        \n        evaluation_info = {\n            'is_new_token': True,\n            'constitutional_analysis': constitutional_analysis,\n            'safety_analysis': safety_analysis,\n            'status_decision': status_decision,\n            'final_status': token_entry.status.value\n        }\n        \n        # Log significant events\n        if token_entry.status == TokenStatus.REJECTED:\n            logger.warning(f\"Token rejected: '{token}' (constitutional: {token_entry.constitutional_score:.3f}, risk: {token_entry.risk_score:.3f})\")\n        \n        return token_entry, evaluation_info\n    \n    def _find_existing_token(self, token: str) -> Optional[TokenEntry]:\n        \"\"\"Find existing token entry across all vocabularies.\n        \n        Args:\n            token: Token to find\n            \n        Returns:\n            Existing token entry or None\n        \"\"\"\n        # Check in order of priority\n        for vocab in [self.base_vocabulary, self.expanded_vocabulary, \n                     self.quarantine_tokens, self.rejected_tokens]:\n            if token in vocab:\n                return vocab[token]\n        return None\n    \n    def _evaluate_constitutional_compliance(self, \n                                          token: str, \n                                          context: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Evaluate constitutional compliance of a token.\n        \n        Args:\n            token: Token to evaluate\n            context: Optional context\n            \n        Returns:\n            Constitutional analysis results\n        \"\"\"\n        token_lower = token.lower()\n        \n        # Check if it's a safe core word\n        is_safe_core = any(token_lower in words for words in self.safe_core_words.values())\n        if is_safe_core:\n            return {\n                'score': 1.0,\n                'is_safe_core': True,\n                'positive_matches': [],\n                'negative_matches': [],\n                'context_boost': 0.0\n            }\n        \n        # Evaluate positive constitutional patterns\n        positive_score = 0.0\n        positive_matches = []\n        \n        for category, patterns in self.constitutional_positive.items():\n            for pattern in patterns:\n                if re.search(pattern, token_lower):\n                    positive_score += 0.2\n                    positive_matches.append(f\"{category}:{pattern}\")\n        \n        # Evaluate negative constitutional patterns\n        negative_score = 0.0\n        negative_matches = []\n        \n        for category, patterns in self.constitutional_negative.items():\n            for pattern in patterns:\n                if re.search(pattern, token_lower):\n                    negative_score += 0.4  # Higher penalty for negative patterns\n                    negative_matches.append(f\"{category}:{pattern}\")\n        \n        # Context-based adjustments\n        context_boost = 0.0\n        if context:\n            context_lower = context.lower()\n            # Educational context gets boost\n            if any(word in context_lower for word in ['learn', 'teach', 'explain', 'understand']):\n                context_boost += 0.1\n            # Safety context gets boost\n            if any(word in context_lower for word in ['safe', 'secure', 'protect', 'ethical']):\n                context_boost += 0.15\n        \n        # Final constitutional score\n        base_score = 0.5  # Neutral starting point\n        final_score = base_score + positive_score - negative_score + context_boost\n        final_score = max(0.0, min(1.0, final_score))\n        \n        return {\n            'score': final_score,\n            'is_safe_core': False,\n            'positive_matches': positive_matches,\n            'negative_matches': negative_matches,\n            'positive_score': positive_score,\n            'negative_score': negative_score,\n            'context_boost': context_boost\n        }\n    \n    def _evaluate_safety_risks(self, \n                              token: str, \n                              context: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Evaluate safety risks of a token.\n        \n        Args:\n            token: Token to evaluate\n            context: Optional context\n            \n        Returns:\n            Safety risk analysis\n        \"\"\"\n        risk_factors = {}\n        total_risk = 0.0\n        \n        # Character-based safety checks\n        for i, pattern in enumerate(self.unsafe_char_patterns):\n            if re.search(pattern, token):\n                risk_name = f'unsafe_chars_{i}'\n                risk_factors[risk_name] = 0.5\n                total_risk += 0.5\n        \n        # Length constraints\n        if len(token) < self.format_constraints['min_length']:\n            risk_factors['too_short'] = 0.3\n            total_risk += 0.3\n        elif len(token) > self.format_constraints['max_length']:\n            risk_factors['too_long'] = 0.4\n            total_risk += 0.4\n        \n        # Digit ratio\n        digit_ratio = sum(1 for c in token if c.isdigit()) / max(1, len(token))\n        if digit_ratio > self.format_constraints['max_digits']:\n            risk_factors['excessive_digits'] = digit_ratio * 0.3\n            total_risk += digit_ratio * 0.3\n        \n        # Uppercase ratio\n        upper_ratio = sum(1 for c in token if c.isupper()) / max(1, len(token))\n        if upper_ratio > self.format_constraints['max_uppercase']:\n            risk_factors['excessive_uppercase'] = upper_ratio * 0.2\n            total_risk += upper_ratio * 0.2\n        \n        # Repetition patterns\n        if re.search(r'(.)\\1{3,}', token):  # Same character 4+ times\n            risk_factors['excessive_repetition'] = 0.4\n            total_risk += 0.4\n        \n        # Mixed scripts (potential encoding attack)\n        has_latin = any(ord(c) < 256 for c in token)\n        has_unicode = any(ord(c) >= 256 for c in token)\n        if has_latin and has_unicode:\n            risk_factors['mixed_scripts'] = 0.3\n            total_risk += 0.3\n        \n        # Final risk score\n        risk_score = min(1.0, total_risk)\n        \n        return {\n            'risk_score': risk_score,\n            'risk_factors': risk_factors,\n            'format_violations': len(risk_factors),\n            'safety_compliant': risk_score < self.config.risk_score_threshold\n        }\n    \n    def _determine_token_status(self, \n                               token_entry: TokenEntry,\n                               constitutional_analysis: Dict[str, Any],\n                               safety_analysis: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Determine the status of a token based on analyses and current mode.\n        \n        Args:\n            token_entry: Token entry being evaluated\n            constitutional_analysis: Constitutional analysis results\n            safety_analysis: Safety analysis results\n            \n        Returns:\n            Status decision information\n        \"\"\"\n        decision_factors = []\n        \n        # Emergency mode - reject almost everything\n        if self.current_mode == VocabularyMode.EMERGENCY:\n            if constitutional_analysis['is_safe_core']:\n                status = TokenStatus.APPROVED\n                decision_factors.append(\"Safe core word in emergency mode\")\n            else:\n                status = TokenStatus.REJECTED\n                decision_factors.append(\"Emergency mode - non-core rejected\")\n        \n        # Static mode - no new tokens\n        elif self.current_mode == VocabularyMode.STATIC:\n            status = TokenStatus.REJECTED\n            decision_factors.append(\"Static mode - no expansion allowed\")\n        \n        # Normal evaluation modes\n        else:\n            # Check safety first\n            if not safety_analysis['safety_compliant']:\n                status = TokenStatus.REJECTED\n                decision_factors.append(f\"Safety risk too high: {token_entry.risk_score:.3f}\")\n            \n            # Check constitutional compliance\n            elif token_entry.constitutional_score < self.config.constitutional_score_threshold:\n                # Mode-specific handling\n                if self.current_mode == VocabularyMode.CONSERVATIVE:\n                    status = TokenStatus.REJECTED\n                    decision_factors.append(f\"Conservative mode - constitutional score too low: {token_entry.constitutional_score:.3f}\")\n                else:\n                    status = TokenStatus.QUARANTINE\n                    decision_factors.append(f\"Quarantined for constitutional review: {token_entry.constitutional_score:.3f}\")\n            \n            # Constitutional violations\n            elif constitutional_analysis['negative_matches']:\n                status = TokenStatus.QUARANTINE\n                decision_factors.append(f\"Constitutional negative patterns: {constitutional_analysis['negative_matches']}\")\n            \n            # Approved cases\n            else:\n                # Mode-specific approval thresholds\n                if self.current_mode == VocabularyMode.CONSERVATIVE:\n                    if token_entry.constitutional_score > 0.8:\n                        status = TokenStatus.APPROVED\n                        decision_factors.append(\"Conservative approval - high constitutional score\")\n                    else:\n                        status = TokenStatus.PENDING\n                        decision_factors.append(\"Conservative mode - pending further evaluation\")\n                \n                elif self.current_mode == VocabularyMode.EXPLORATORY:\n                    if token_entry.constitutional_score > 0.3:\n                        status = TokenStatus.APPROVED\n                        decision_factors.append(\"Exploratory approval - moderate constitutional score\")\n                    else:\n                        status = TokenStatus.QUARANTINE\n                        decision_factors.append(\"Exploratory quarantine - low constitutional score\")\n                \n                else:  # Balanced mode\n                    if token_entry.constitutional_score >= self.config.constitutional_score_threshold:\n                        status = TokenStatus.APPROVED\n                        decision_factors.append(\"Balanced approval - meets constitutional threshold\")\n                    else:\n                        status = TokenStatus.PENDING\n                        decision_factors.append(\"Balanced mode - pending threshold evaluation\")\n        \n        return {\n            'status': status,\n            'decision_factors': decision_factors,\n            'mode_applied': self.current_mode.value\n        }\n    \n    def _store_token(self, token_entry: TokenEntry):\n        \"\"\"Store token in appropriate vocabulary collection.\n        \n        Args:\n            token_entry: Token entry to store\n        \"\"\"\n        token = token_entry.token\n        \n        # Remove from other collections if it exists\n        for vocab in [self.base_vocabulary, self.expanded_vocabulary, \n                     self.quarantine_tokens, self.rejected_tokens]:\n            if token in vocab:\n                del vocab[token]\n        \n        # Store in appropriate collection\n        if token_entry.status == TokenStatus.APPROVED:\n            if len(self.expanded_vocabulary) < self.config.max_expansion_size:\n                self.expanded_vocabulary[token] = token_entry\n            else:\n                # Vocabulary full - need to make room or reject\n                self._handle_vocabulary_overflow(token_entry)\n        \n        elif token_entry.status == TokenStatus.CONSTITUTIONAL:\n            self.base_vocabulary[token] = token_entry\n        \n        elif token_entry.status == TokenStatus.PENDING:\n            self.expanded_vocabulary[token] = token_entry\n        \n        elif token_entry.status == TokenStatus.QUARANTINE:\n            self.quarantine_tokens[token] = token_entry\n        \n        elif token_entry.status == TokenStatus.REJECTED:\n            self.rejected_tokens[token] = token_entry\n    \n    def _handle_vocabulary_overflow(self, new_token_entry: TokenEntry):\n        \"\"\"Handle vocabulary size overflow.\n        \n        Args:\n            new_token_entry: New token that would exceed vocabulary size\n        \"\"\"\n        # Find least valuable token to replace\n        if self.expanded_vocabulary:\n            # Score existing tokens (frequency * constitutional_score)\n            token_scores = []\n            for token, entry in self.expanded_vocabulary.items():\n                score = entry.frequency * entry.constitutional_score\n                token_scores.append((score, token, entry))\n            \n            # Sort by score (lowest first)\n            token_scores.sort(key=lambda x: x[0])\n            \n            # Compare new token with lowest scored token\n            new_score = new_token_entry.frequency * new_token_entry.constitutional_score\n            \n            if token_scores and new_score > token_scores[0][0]:\n                # Replace lowest scored token\n                _, old_token, old_entry = token_scores[0]\n                \n                # Move old token to rejected\n                old_entry.status = TokenStatus.REJECTED\n                self.rejected_tokens[old_token] = old_entry\n                del self.expanded_vocabulary[old_token]\n                \n                # Add new token\n                self.expanded_vocabulary[new_token_entry.token] = new_token_entry\n                \n                logger.info(f\"Vocabulary overflow: replaced '{old_token}' (score {token_scores[0][0]:.3f}) with '{new_token_entry.token}' (score {new_score:.3f})\")\n            else:\n                # New token not good enough\n                new_token_entry.status = TokenStatus.REJECTED\n                self.rejected_tokens[new_token_entry.token] = new_token_entry\n        else:\n            # No existing tokens to replace\n            new_token_entry.status = TokenStatus.REJECTED\n            self.rejected_tokens[new_token_entry.token] = new_token_entry\n    \n    def _generate_token_id(self, token: str) -> int:\n        \"\"\"Generate unique token ID.\n        \n        Args:\n            token: Token to generate ID for\n            \n        Returns:\n            Unique token ID\n        \"\"\"\n        # Use hash-based ID generation with collision handling\n        token_hash = int(hashlib.md5(token.encode('utf-8')).hexdigest()[:8], 16)\n        token_id = self.config.base_vocabulary_size + (token_hash % self.config.max_expansion_size)\n        \n        # Handle collisions\n        while any(entry.token_id == token_id for vocab in [self.base_vocabulary, self.expanded_vocabulary, \n                                                           self.quarantine_tokens, self.rejected_tokens]\n                 for entry in vocab.values()):\n            token_id = (token_id + 1) % (self.config.base_vocabulary_size + self.config.max_expansion_size)\n            if token_id < self.config.base_vocabulary_size:\n                token_id = self.config.base_vocabulary_size  # Ensure in expansion range\n        \n        return token_id\n    \n    def promote_quarantine_tokens(self) -> Dict[str, Any]:\n        \"\"\"Review and potentially promote tokens from quarantine.\n        \n        Returns:\n            Promotion results\n        \"\"\"\n        promoted = []\n        still_quarantined = []\n        rejected = []\n        \n        for token, entry in list(self.quarantine_tokens.items()):\n            entry.quarantine_count += 1\n            \n            # Re-evaluate after quarantine period\n            if entry.quarantine_count >= self.config.quarantine_period:\n                # Re-evaluate constitutional score based on usage patterns\n                if entry.frequency >= self.config.min_frequency_threshold:\n                    # Promote if it has gained sufficient usage\n                    entry.status = TokenStatus.APPROVED\n                    self.expanded_vocabulary[token] = entry\n                    del self.quarantine_tokens[token]\n                    promoted.append(token)\n                    logger.info(f\"Quarantine token promoted: '{token}' (freq={entry.frequency})\")\n                else:\n                    # Reject if still low usage\n                    entry.status = TokenStatus.REJECTED\n                    self.rejected_tokens[token] = entry\n                    del self.quarantine_tokens[token]\n                    rejected.append(token)\n            else:\n                still_quarantined.append(token)\n        \n        return {\n            'promoted': promoted,\n            'rejected': rejected,\n            'still_quarantined': still_quarantined,\n            'total_processed': len(promoted) + len(rejected)\n        }\n    \n    def set_vocabulary_mode(self, mode: VocabularyMode):\n        \"\"\"Set the vocabulary adaptation mode.\n        \n        Args:\n            mode: New vocabulary mode\n        \"\"\"\n        old_mode = self.current_mode\n        self.current_mode = mode\n        \n        # Handle mode-specific actions\n        if mode == VocabularyMode.EMERGENCY:\n            self.stats['emergency_lockdowns'] += 1\n            logger.warning(f\"EMERGENCY: Vocabulary locked down - only safe core words allowed\")\n        \n        logger.info(f\"Vocabulary mode changed: {old_mode.value} -> {mode.value}\")\n    \n    def reset_vocabulary_expansion(self, preserve_approved: bool = True):\n        \"\"\"Reset vocabulary expansion state.\n        \n        Args:\n            preserve_approved: Whether to preserve approved expanded tokens\n        \"\"\"\n        if preserve_approved:\n            # Keep approved tokens, clear others\n            approved_tokens = {k: v for k, v in self.expanded_vocabulary.items() \n                             if v.status == TokenStatus.APPROVED}\n            self.expanded_vocabulary = approved_tokens\n        else:\n            # Clear all expanded vocabulary\n            self.expanded_vocabulary.clear()\n        \n        # Clear quarantine and rejected\n        self.quarantine_tokens.clear()\n        self.rejected_tokens.clear()\n        \n        # Reset statistics\n        self.stats['vocabulary_resets'] += 1\n        \n        logger.info(f\"Vocabulary expansion reset (preserve_approved={preserve_approved})\")\n    \n    def get_vocabulary_status(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive vocabulary status.\n        \n        Returns:\n            Vocabulary status information\n        \"\"\"\n        return {\n            'configuration': {\n                'mode': self.current_mode.value,\n                'base_size': self.config.base_vocabulary_size,\n                'max_expansion': self.config.max_expansion_size,\n                'constitutional_threshold': self.config.constitutional_score_threshold\n            },\n            'current_state': {\n                'base_vocabulary_size': len(self.base_vocabulary),\n                'expanded_vocabulary_size': len(self.expanded_vocabulary),\n                'quarantine_size': len(self.quarantine_tokens),\n                'rejected_size': len(self.rejected_tokens),\n                'expansion_utilization': len(self.expanded_vocabulary) / self.config.max_expansion_size\n            },\n            'statistics': self.stats.copy(),\n            'recent_activity': {\n                'tokens_in_quarantine': list(self.quarantine_tokens.keys())[:10],  # First 10\n                'recently_approved': [token for token, entry in self.expanded_vocabulary.items() \n                                    if entry.status == TokenStatus.APPROVED][:10],\n                'recently_rejected': list(self.rejected_tokens.keys())[-10:]  # Last 10\n            }\n        }\n    \n    def generate_vocabulary_report(self) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive vocabulary adaptation report.\n        \n        Returns:\n            Detailed vocabulary analysis report\n        \"\"\"\n        # Analyze approval rates by constitutional score ranges\n        approved_tokens = [entry for entry in self.expanded_vocabulary.values() \n                          if entry.status == TokenStatus.APPROVED]\n        \n        if approved_tokens:\n            constitutional_scores = [entry.constitutional_score for entry in approved_tokens]\n            avg_constitutional_score = np.mean(constitutional_scores)\n            \n            frequency_distribution = Counter([entry.frequency for entry in approved_tokens])\n        else:\n            avg_constitutional_score = 0.0\n            frequency_distribution = Counter()\n        \n        # Analyze rejection reasons\n        rejection_analysis = {}\n        for token, entry in self.rejected_tokens.items():\n            if entry.constitutional_score < self.config.constitutional_score_threshold:\n                rejection_analysis['low_constitutional'] = rejection_analysis.get('low_constitutional', 0) + 1\n            if entry.risk_score > self.config.risk_score_threshold:\n                rejection_analysis['high_risk'] = rejection_analysis.get('high_risk', 0) + 1\n        \n        return {\n            'status': 'active',\n            'mode': self.current_mode.value,\n            'vocabulary_metrics': {\n                'total_tokens_managed': len(self.base_vocabulary) + len(self.expanded_vocabulary) + \n                                      len(self.quarantine_tokens) + len(self.rejected_tokens),\n                'expansion_efficiency': len(self.expanded_vocabulary) / max(1, self.stats['tokens_evaluated']),\n                'approval_rate': self.stats['tokens_approved'] / max(1, self.stats['tokens_evaluated']),\n                'rejection_rate': self.stats['tokens_rejected'] / max(1, self.stats['tokens_evaluated'])\n            },\n            'quality_analysis': {\n                'avg_constitutional_score': avg_constitutional_score,\n                'frequency_distribution': dict(frequency_distribution.most_common(10)),\n                'rejection_reasons': rejection_analysis\n            },\n            'statistics': self.stats.copy(),\n            'recommendations': self._generate_vocabulary_recommendations()\n        }\n    \n    def _generate_vocabulary_recommendations(self) -> List[str]:\n        \"\"\"Generate vocabulary management recommendations.\n        \n        Returns:\n            List of recommendations\n        \"\"\"\n        recommendations = []\n        \n        # Check approval rate\n        approval_rate = self.stats['tokens_approved'] / max(1, self.stats['tokens_evaluated'])\n        if approval_rate < 0.1:\n            recommendations.append(f\"Very low approval rate ({approval_rate:.3f}) - consider more permissive mode\")\n        elif approval_rate > 0.8:\n            recommendations.append(f\"Very high approval rate ({approval_rate:.3f}) - consider stricter evaluation\")\n        \n        # Check quarantine backlog\n        if len(self.quarantine_tokens) > 100:\n            recommendations.append(f\"Large quarantine backlog ({len(self.quarantine_tokens)}) - review quarantine policies\")\n        \n        # Check expansion utilization\n        utilization = len(self.expanded_vocabulary) / self.config.max_expansion_size\n        if utilization > 0.9:\n            recommendations.append(f\"Vocabulary expansion near capacity ({utilization:.3f}) - consider increasing limit or pruning\")\n        \n        # Check emergency lockdowns\n        if self.stats['emergency_lockdowns'] > 3:\n            recommendations.append(\"Multiple emergency lockdowns detected - review safety thresholds\")\n        \n        # Check constitutional compliance\n        constitutional_violations = len([entry for entry in self.expanded_vocabulary.values() \n                                       if entry.constitutional_score < self.config.constitutional_score_threshold])\n        if constitutional_violations > len(self.expanded_vocabulary) * 0.1:\n            recommendations.append(f\"High constitutional violation rate in vocabulary - review approval process\")\n        \n        if not recommendations:\n            recommendations.append(\"Adaptive vocabulary operating within normal parameters\")\n        \n        return recommendations